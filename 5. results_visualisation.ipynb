{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spark_session_manager import SparkSessionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_emotions = pd.read_parquet(\"data/results/emotion-english-distilroberta-base.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame\n",
    "emotion_views = df.groupBy('emotion').agg(F.mean('view_count').alias('mean_view_count'))\\\n",
    "    .orderBy(F.desc('mean_view_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "emotion_views_pd = emotion_views.toPandas()\n",
    "\n",
    "# Normalize 'mean_view_count' for color mapping\n",
    "max_views = emotion_views_pd['mean_view_count'].max()\n",
    "min_views = emotion_views_pd['mean_view_count'].min()\n",
    "emotion_views_pd['normalized_views'] = (\n",
    "    emotion_views_pd['mean_view_count'] - min_views) / (max_views - min_views)\n",
    "\n",
    "# Use Plotly Express to generate a continuous color scale\n",
    "\n",
    "\n",
    "# Generate a continuous colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "colors = [cmap(x) for x in emotion_views_pd['normalized_views']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head\n",
    "df_subset_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a bar chart with colors based on 'mean_view_count'\n",
    "fig = go.Figure(data=go.Bar(\n",
    "    x=df_subset_pd['emotion'],\n",
    "    y=df_subset_pd['view_count'],\n",
    "    marker_color=colors  # Use the generated colors\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Views by Emotion',\n",
    "    xaxis_title='Emotion',\n",
    "    yaxis_title='Average Views',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# change theme to white\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# save the plot as a .html file\n",
    "fig.write_html(\n",
    "    \"charts/emotions/mean_views_by_emotion.html\")\n",
    "\n",
    "# save as png\n",
    "fig.write_image(\n",
    "    \"charts/emotions/mean_views_by_emotion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'spark' is your SparkSession\n",
    "# Example: spark = SparkSession.builder.appName(\"YourAppName\").getOrCreate()\n",
    "\n",
    "# Adjust the DataFrame read if 'df' is not already a PySpark DataFrame\n",
    "# df = spark.read.csv(\"path/to/your.csv\", header=True, inferSchema=True)\n",
    "\n",
    "emotion_views = df.groupBy(\"emotion\") \\\n",
    "    .agg(F.mean(\"view_count\").alias(\"mean_view_count\")) \\\n",
    "    .orderBy(F.desc(\"mean_view_count\"))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "emotion_views_pd = emotion_views.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Normalize 'mean_view_count' between 0 and 1\n",
    "min_view_count = emotion_views_pd['mean_view_count'].min()\n",
    "max_view_count = emotion_views_pd['mean_view_count'].max()\n",
    "emotion_views_pd['normalized_view_count'] = (\n",
    "    emotion_views_pd['mean_view_count'] - min_view_count) / (max_view_count - min_view_count)\n",
    "\n",
    "# Use continuous color scale\n",
    "colors = px.colors.sequential.Viridis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# import plotly settings and set black theme\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Convert normalized view counts to colors using matplotlib's Viridis colormap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "emotion_views_pd['color'] = emotion_views_pd['normalized_view_count'].apply(\n",
    "    lambda x: mcolors.to_hex(cmap(x)))\n",
    "\n",
    "# Now plot with Plotly, using the interpolated colors\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, row in emotion_views_pd.iterrows():\n",
    "    fig.add_trace(go.Bar(x=[row['emotion']], y=[row['mean_view_count']],\n",
    "                         marker_color=row['color']))\n",
    "\n",
    "fig.update_layout(title='Average Views by Emotion',\n",
    "                  xaxis_title='Emotion',\n",
    "                  yaxis_title='Average Views',\n",
    "                  template='plotly_dark',\n",
    "                  showlegend=False)\n",
    "\n",
    "\n",
    "# save as html, cdn\n",
    "# fig.write_html(\"charts/emotions/mean_views_by_emotion.html\")\n",
    "\n",
    "# save svg\n",
    "fig.write_image(\n",
    "    \"charts/emotions/mean_views_by_emotion.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Distribution Across Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count the number of videos for each combination of category and emotion\n",
    "emotion_category_distribution = df.groupBy(\"categoryId\", \"emotion\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"counts\") \\\n",
    "    .toPandas()\n",
    "\n",
    "\n",
    "# Create a stacked bar chart using the Pandas DataFrame\n",
    "fig = px.bar(emotion_category_distribution,\n",
    "             x='categoryId',\n",
    "             y='counts',\n",
    "             color='emotion',\n",
    "             title=\"Emotion Distribution Across Categories\",\n",
    "             labels={'counts': 'Number of Videos', 'categoryId': 'Category ID'})\n",
    "\n",
    "fig.update_layout(template='plotly_dark')\n",
    "\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/emotions/emotion_distribution_across_categories.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement Metrics by Emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average controversy index by emotion\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "average_controversy_by_emotion = df.groupBy(\"emotion\") \\\n",
    "    .agg(F.avg(\"controversy_index\").alias(\"average_controversy\")) \\\n",
    "    .toPandas()\n",
    "\n",
    "\n",
    "# Normalize the 'average_controversy' to a 0-1 range\n",
    "min_controversy = average_controversy_by_emotion['average_controversy'].min()\n",
    "max_controversy = average_controversy_by_emotion['average_controversy'].max()\n",
    "average_controversy_by_emotion['normalized_controversy'] = (\n",
    "    average_controversy_by_emotion['average_controversy'] - min_controversy) / (max_controversy - min_controversy)\n",
    "\n",
    "\n",
    "# Get the Viridis color scale\n",
    "viridis = px.colors.sequential.Viridis\n",
    "\n",
    "# Map normalized controversy to colors\n",
    "average_controversy_by_emotion['color'] = average_controversy_by_emotion['normalized_controversy'].apply(\n",
    "    lambda x: viridis[int(x * (len(viridis) - 1))])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for index, row in average_controversy_by_emotion.iterrows():\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[row['emotion']],\n",
    "        y=[row['average_controversy']],\n",
    "        marker_color=row['color'],  # Use the Viridis color mapped above\n",
    "        name=row['emotion']\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average Controversy Index by Emotion with Viridis Color Scale',\n",
    "    xaxis_title='Emotion',\n",
    "    yaxis_title='Average Controversy Index',\n",
    "    template='plotly_dark',\n",
    "    showlegend=False  # Hide the legend if not necessary\n",
    ")\n",
    "\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/emotions/average_controversy_index_by_emotion.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Engagement Metrics by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Average Engagement Metrics in PySpark\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from palettable import colorbrewer\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Assuming df is a PySpark DataFrame with your data\n",
    "metrics = df.groupBy('categoryId') \\\n",
    "    .agg(\n",
    "        avg('comment_rate').alias('avg_comment_rate'),\n",
    "        avg('dislike_rate').alias('avg_dislike_rate'),\n",
    "        avg('dislike_ratio').alias('avg_dislike_ratio'),\n",
    "        avg('controversy_index').alias('avg_controversy_index')\n",
    ")\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "metrics_pd = metrics.toPandas()\n",
    "\n",
    "# ------------------------- normalize for radar chart ------------------------ #\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "metrics_normalized = pd.DataFrame(scaler.fit_transform(metrics_pd.iloc[:, 1:]),\n",
    "                                  columns=metrics_pd.columns[1:],\n",
    "                                  index=metrics_pd['categoryId'])\n",
    "\n",
    "# Reset index to keep 'categoryId' as a column for plotting\n",
    "metrics_normalized.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# use color pallette from brewer\n",
    "\n",
    "sequential_palette = colorbrewer.sequential.YlGnBu_9.hex_colors\n",
    "\n",
    "\n",
    "# Plot Radar Chart\n",
    "\n",
    "\n",
    "categories = list(metrics_normalized.columns[1:])\n",
    "N = len(categories)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "for index, row in metrics_normalized.iterrows():\n",
    "    data = metrics_normalized.iloc[index].drop('categoryId').tolist()\n",
    "    data += data[:1]\n",
    "    ax.plot(angles, data, linewidth=2,\n",
    "            linestyle='solid', label=row['categoryId'])\n",
    "    ax.fill(angles, data, alpha=0.1)\n",
    "\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "# # add names of labels at the end of each line\n",
    "# for i in range(len(metrics_normalized)):\n",
    "#     ax.text(angles[-1], data[-1], metrics_normalized['categoryId'][i], fontsize=12, color=sequential_palette[i])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# save as svg\n",
    "\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/eng_metrics/average_engagement_metrics.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assume metrics_normalized_df has been normalized and contains categories as rows\n",
    "# and metrics as columns, as per your previous data structure.\n",
    "\n",
    "# Get unique categories\n",
    "unique_categories = metrics_normalized['categoryId'].unique()\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "num_rows = int(len(unique_categories) ** 0.5)\n",
    "num_cols = (len(unique_categories) + num_rows -\n",
    "            1) // num_rows  # Ceiling division\n",
    "\n",
    "# Create subplots, each subplot is a radar chart\n",
    "fig = make_subplots(\n",
    "    rows=num_rows, cols=num_cols,\n",
    "    specs=[[{'type': 'polar'}] * num_cols] * num_rows,\n",
    "    subplot_titles=unique_categories\n",
    ")\n",
    "\n",
    "# Plot each category in a separate radar chart\n",
    "for i, category in enumerate(unique_categories):\n",
    "    category_data = metrics_normalized[metrics_normalized['categoryId']\n",
    "                                       == category].iloc[0]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=category_data[metrics].tolist() + [category_data[metrics[0]]],\n",
    "            theta=metrics,\n",
    "            fill='toself',\n",
    "            name=category\n",
    "        ),\n",
    "        row=(i // num_cols) + 1,  # Row in subplot grid\n",
    "        col=(i % num_cols) + 1    # Col in subplot grid\n",
    "    )\n",
    "\n",
    "# Update layout for all subplots\n",
    "fig.update_layout(\n",
    "    title='Average Engagement Metrics by Category',\n",
    "    template='plotly_dark',\n",
    "    polar=dict(radialaxis=dict(visible=True)),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# hide avg_controversy_index etc labels outside radar\n",
    "fig.update_polars(radialaxis=dict(visible=True, tickangle=45))\n",
    "\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/eng_metrics/radar_chart_average_engagement_metrics.svg\", width=1900, height=1200)\n",
    "\n",
    "# save as html\n",
    "fig.write_html(\n",
    "    \"charts/eng_metrics/radar_chart_average_engagement_metrics.html\", auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments rate = comments / views for category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame (if not already in Pandas)\n",
    "import plotly.figure_factory as ff\n",
    "df_pd = df.select(\"view_count\", \"likes\", \"dislikes\", \"comment_count\",\n",
    "                                                            \"comment_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\").toPandas()\n",
    "\n",
    "# Calculate Correlation Matrix\n",
    "corr_matrix = df_pd.corr()\n",
    "\n",
    "# Plot Heatmap with Plotly\n",
    "\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=list(corr_matrix.columns),\n",
    "    y=list(corr_matrix.index),\n",
    "    annotation_text=corr_matrix.round(2).values,\n",
    "    colorscale='Viridis',\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Correlation Heatmap of Engagement Metrics',\n",
    "                  xaxis_title='Metrics',\n",
    "                  yaxis_title='Metrics',\n",
    "                  template='plotly_dark')\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/eng_metrics/Correlation_Heatmap_of_Engagement_Metrics.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_pd is your Pandas DataFrame\n",
    "corr_matrix = df_pd.corr()\n",
    "\n",
    "# Mask to display only upper half\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Heatmap\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=corr_matrix.where(mask).values,\n",
    "    x=list(corr_matrix.columns),\n",
    "    y=list(corr_matrix.index),\n",
    "    annotation_text=corr_matrix.where(mask).round(2).astype(str).values,\n",
    "    colorscale='Viridis',\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "# Update layout to include definitions\n",
    "metric_definitions = \"\"\"\n",
    "<b>Definitions</b>:\n",
    "<br>Comment Rate = comments / views || Dislike Rate = dislikes / view\n",
    "<br>Dislike Ratio = dislikes / (likes + dislikes) || Controversy Index = comment rate * dislike ratio\n",
    "\"\"\"\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Correlation Heatmap of Engagement Metrics<br>',\n",
    "    xaxis=dict(title='Metrics'),\n",
    "    yaxis=dict(title='Metrics'),\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# inert annotations at the bottom\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=metric_definitions,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.1, y=-0.1, showarrow=False,\n",
    "    font=dict(size=14, color=\"white\")\n",
    ")\n",
    "\n",
    "# save as svg\n",
    "fig.write_image(\n",
    "    \"charts/eng_metrics/Correlation_Heatmap_of_Engagement_Metrics2.svg\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of emotions and engagement metrics\n",
    "\n",
    "# Calculate Correlation Matrix\n",
    "corr_matrix = df.select(\"view_count\", \"likes\", \"dislikes\", \"comment_count\", \"comment_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\") \\\n",
    "    .toPandas().corr()\n",
    "\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create df for emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions per category\n",
    "emotions_per_category = df.groupBy(\n",
    "    'categoryId', 'emotion').count().orderBy('categoryId', 'emotion')\n",
    "\n",
    "# convert to pandas\n",
    "emotions_per_category_pd = emotions_per_category.toPandas()\n",
    "\n",
    "# head\n",
    "emotions_per_category_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show whole dataframe, do not truncate\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# emotions_per_category_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart emotions per category\n",
    "# use stacked bar plot\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a figure with subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\n",
    "    \"Emotions per Category\", \"Emotions per Category (Normalized)\"))\n",
    "\n",
    "# Create a bar plot for the absolute counts\n",
    "\n",
    "for i, category in enumerate(emotions_per_category_pd['categoryId'].unique()):\n",
    "    data = emotions_per_category_pd[emotions_per_category_pd['categoryId'] == category]\n",
    "    fig.add_trace(go.Bar(x=data['emotion'],\n",
    "                  y=data['count'], name=category), row=1, col=1)\n",
    "\n",
    "# Create a bar plot for the normalized counts\n",
    "for i, category in enumerate(emotions_per_category_pd['categoryId'].unique()):\n",
    "    data = emotions_per_category_pd[emotions_per_category_pd['categoryId'] == category]\n",
    "    fig.add_trace(go.Bar(x=data['emotion'], y=data['count'] /\n",
    "                  data['count'].sum(), name=category), row=1, col=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sankey chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pySankey.sankey import sankey\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_subset_pd is your DataFrame with 'emotion' and 'categoryId' columns ready for plotting\n",
    "\n",
    "# Create a Sankey chart\n",
    "sankey(df_subset_pd['emotion'],\n",
    "       df_subset_pd['categoryId'], aspect=20, fontsize=10)\n",
    "\n",
    "# Get current figure\n",
    "fig = plt.gcf()\n",
    "\n",
    "# Set the size of the figure in inches [width, height]\n",
    "# Adjust these values as needed to avoid label collision\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "# Set the color of the background to black\n",
    "fig.set_facecolor(\"black\")\n",
    "\n",
    "# set title \"Emotions per Category (Sankey Diagram)\"\n",
    "plt.title(\"Emotions per Category (Sankey Diagram)\", fontsize=14, color=\"white\")\n",
    "\n",
    "# set color of the labels to white\n",
    "plt.rcParams['text.color'] = 'white'\n",
    "\n",
    "# Save the figure to a file, adjust the dpi to increase the resolution\n",
    "fig.savefig(\"charts/sankey-emotions-per-category.png\", bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "# save plt to file 'html\n",
    "# plt.savefig(\"charts/sankey-emotions-per-category.html\", format='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-importing necessary library for the Sankey diagram\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming a simplified version of the provided data for demonstration purposes\n",
    "\n",
    "# Correcting the data input issue and focusing on including colors for the emotions\n",
    "\n",
    "# Simplified data structure for creating the Sankey diagram with colored emotions\n",
    "category_ids = emotions_per_category_pd['categoryId'].astype(\n",
    "    'category').cat.codes\n",
    "emotion_ids = emotions_per_category_pd['emotion'].astype(\n",
    "    'category').cat.codes + category_ids.max() + 1  # Offset by max category id\n",
    "counts = emotions_per_category_pd['count']\n",
    "\n",
    "# Unique categories and emotions for labeling\n",
    "unique_categories = emotions_per_category_pd['categoryId'].astype(\n",
    "    'category').cat.categories\n",
    "\n",
    "unique_emotions = emotions_per_category_pd['emotion'].astype(\n",
    "    'category').cat.categories\n",
    "\n",
    "node_colors = [emotions_palette[emotion]\n",
    "               if emotion in emotions_palette else 'grey' for emotion in unique_emotions]\n",
    "\n",
    "# Creating the Sankey diagram with colors\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color='black', width=0.5),\n",
    "        label=list(unique_categories) + list(unique_emotions),\n",
    "        # Default grey for categories, colored for emotions\n",
    "        color=['grey']*len(unique_categories) + node_colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=category_ids,\n",
    "        target=emotion_ids,\n",
    "        value=counts\n",
    "    ))])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"YouTube Sentiment Analysis by Category and Emotion with Color\", font_size=12)\n",
    "\n",
    "# save as html, use cdn\n",
    "fig.write_html(\"charts/sankey.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## proportions of emotions within each YouTube category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming emotions_per_category_pd is your DataFrame\n",
    "\n",
    "# Pivot the DataFrame to get a matrix where categories are rows, emotions are columns, and values are counts\n",
    "pivot_df = emotions_per_category_pd.pivot(\n",
    "    index='categoryId', columns='emotion', values='count').fillna(0)\n",
    "\n",
    "# Normalize the counts by row to get proportions\n",
    "proportions = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot a 100% stacked bar chart\n",
    "proportions.plot(kind='bar', stacked=True, figsize=(10, 7))\n",
    "\n",
    "# Configure the plot with titles, labels, etc.\n",
    "plt.title('Proportion of Emotions per YouTube Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Proportion of Emotions')\n",
    "plt.legend(title='Emotion')\n",
    "plt.tight_layout()\n",
    "\n",
    "# change color of legend\n",
    "plt.legend(title='Emotion', title_fontsize='12',\n",
    "           fontsize='12', facecolor='black', edgecolor='black', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to get a matrix where categories are rows, emotions are columns, and values are counts\n",
    "# pivot_df = emotions_per_category_pd.pivot(index='categoryId', columns='emotion', values='count').fillna(0)\n",
    "\n",
    "# Normalize the counts by row to get proportions\n",
    "proportions = pivot_df.div(pivot_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Create a list of traces for the stacked bar chart, one for each emotion\n",
    "traces = []\n",
    "\n",
    "# Pivot the DataFrame to have categories as rows and emotions as columns, filling NaNs with 0\n",
    "category_emotion_counts = emotions_per_category_pd.pivot_table(\n",
    "    index='categoryId',\n",
    "    columns='emotion',\n",
    "    values='count',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Calculate the proportions\n",
    "category_emotion_proportions = category_emotion_counts.div(\n",
    "    category_emotion_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Create the plotly figure with one trace for each emotion column\n",
    "fig = go.Figure()\n",
    "\n",
    "for emotion in category_emotion_proportions.columns:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=emotion,\n",
    "        x=category_emotion_proportions.index,\n",
    "        y=category_emotion_proportions[emotion],\n",
    "        hoverinfo='none'  # Disable default hoverinfo\n",
    "    ))\n",
    "\n",
    "# Update the layout of the figure to stack the bars and adjust the y-axis to show percentage\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Proportion of Emotions per YouTube Category',\n",
    "    xaxis_title='Category',\n",
    "    yaxis=dict(\n",
    "        title='Proportion',\n",
    "        tickformat=',.0%'\n",
    "    ),\n",
    "    hoverlabel=dict(  # Customize hover label\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    ),\n",
    "    legend_title_text='Emotion'\n",
    ")\n",
    "\n",
    "# Define the custom hovertemplate with HTML\n",
    "# {emotion} in {category}: {y:.2%} <extra></extra>\n",
    "hovertemplate = \"<b>%{y:.2%}</b> of <b>%{x}</b> emits <b>%{fullData.name}</b><extra></extra>\"\n",
    "\n",
    "# Apply custom hovertemplate to each trace\n",
    "for trace in fig.data:\n",
    "    trace.hovertemplate = hovertemplate\n",
    "\n",
    "# change colors of the bars using the emotions_palette\n",
    "for i, emotion in enumerate(category_emotion_proportions.columns):\n",
    "    fig.data[i].marker.color = emotions_palette[emotion]\n",
    "\n",
    "# Save the figure to an HTML file\n",
    "fig.write_html(\"charts/stacked_bar.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement metrics by emotion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of eng. metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "## Removing Rows with Null Values\n",
    "df = df.na.drop(subset=[\"comment_count\", \"view_count\", \"likes\", \"dislikes\"]) # This approach will help ensure that the VectorAssembler does not encounter null values, which should prevent the Py4JJavaError you're seeing.\n",
    "\n",
    "## Filling Null Values\n",
    "# df_subset = df_subset.na.fill(value=0, subset=[\"comment_count\", \"view_count\", \"likes\", \"dislikes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 3: Assemble features\n",
    "feature_columns = [\"comment_rate\", \"like_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using handleInvalid Parameter in VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"comment_rate\", \"like_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# Transform the DataFrame to include a features vector column\n",
    "df_features = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and train the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"view_count\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = df_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an evaluator for RMSE\n",
    "evaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"view_count\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n",
    "\n",
    "# If you want to calculate R-squared\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"view_count\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data =\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks\n",
    "\n",
    "## coefficients with their names\n",
    "print(\"## Coefficients\")\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(feature, \":\", lr_model.coefficients[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Handling null values (assuming 'df_subset' already has engagement metrics calculated)\n",
    "df = df.na.drop()  # Dropping rows with any nulls\n",
    "\n",
    "# RFormula to automatically include interactions (example formula, adjust accordingly)\n",
    "r_formula = RFormula(formula=\"view_count ~ categoryId:emotion + comment_rate + dislike_rate + dislike_ratio + controversy_index + categoryId + emotion\")\n",
    "\n",
    "# Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"view_count\")\n",
    "\n",
    "# Define Pipeline\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an evaluator for RMSE\n",
    "evaluator_rmse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"view_count\", metricName=\"rmse\")\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data =\", rmse)\n",
    "\n",
    "# If you want to calculate R-squared\n",
    "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"view_count\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2) on test data =\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is the fitted pipeline model\n",
    "lr_model = model.stages[-1]  # The last stage in the pipeline is the LinearRegression model\n",
    "\n",
    "# Extracting coefficients and intercept\n",
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "print(\"Intercept: \", intercept)\n",
    "print(\"Coefficients: \")\n",
    "    \n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(feature, \":\", coefficients[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stewise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize a list to keep track of variables added and their R²\n",
    "selected_variables = []\n",
    "current_r2 = -float(\"inf\")\n",
    "improvement = True\n",
    "\n",
    "# List all potential predictors\n",
    "potential_predictors = [\"comment_rate\", \"like_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\"]\n",
    "\n",
    "while improvement:\n",
    "    improvement = False\n",
    "    best_r2_for_step = current_r2\n",
    "    \n",
    "    for predictor in potential_predictors:\n",
    "        # Temporary model with the current selected variables plus the new potential predictor\n",
    "        assembler = VectorAssembler(inputCols=selected_variables + [predictor], outputCol=\"features\")\n",
    "        \n",
    "        # Fit and evaluate the model\n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"view_count\")\n",
    "        pipeline = Pipeline(stages=[assembler, lr])\n",
    "        model = pipeline.fit(train_data)  # Assuming train_data is already defined\n",
    "        predictions = model.transform(test_data)  # Assuming test_data is already defined\n",
    "        \n",
    "        evaluator = RegressionEvaluator(labelCol=\"view_count\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        r2 = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if r2 > best_r2_for_step:\n",
    "            best_r2_for_step = r2\n",
    "            best_predictor = predictor\n",
    "            improvement = True\n",
    "            \n",
    "    # If there was an improvement, update the list of selected variables and the current best R²\n",
    "    if improvement:\n",
    "        selected_variables.append(best_predictor)\n",
    "        current_r2 = best_r2_for_step\n",
    "        potential_predictors.remove(best_predictor)\n",
    "        print(f\"Added {best_predictor}. New R² is {best_r2_for_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the selected variables\n",
    "print(\"Selected variables:\", selected_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming df_subset is your DataFrame that includes 'categoryId', 'emotion', and engagement metrics\n",
    "\n",
    "# List of engagement metrics to include as features\n",
    "feature_cols = [\"comment_rate\", \"like_rate\", \"dislike_rate\", \"dislike_ratio\", \"controversy_index\"]\n",
    "\n",
    "# Initialize an evaluator for R2\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"view_count\", metricName=\"r2\")\n",
    "\n",
    "# Retrieve unique categories and emotions\n",
    "categories = df.select(\"categoryId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "emotions = df.select(\"emotion\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "results = []\n",
    "\n",
    "for category in categories:\n",
    "    for emotion in emotions:\n",
    "        # Filter the DataFrame for the current category and emotion\n",
    "        df_filtered = df.filter((df.categoryId == category) & (df.emotion == emotion))\n",
    "        \n",
    "        # Check if the filtered DataFrame is not empty\n",
    "        if df_filtered.count() > 0:\n",
    "            assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "            lr = LinearRegression(featuresCol=\"features\", labelCol=\"view_count\")\n",
    "            \n",
    "            # Define the pipeline\n",
    "            pipeline = Pipeline(stages=[assembler, lr])\n",
    "            \n",
    "            # Split the data\n",
    "            train_data, test_data = df_filtered.randomSplit([0.7, 0.3], seed=42)\n",
    "            \n",
    "            # Fit the model\n",
    "            model = pipeline.fit(train_data)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.transform(test_data)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            r2 = evaluator.evaluate(predictions)\n",
    "            \n",
    "            # Store the results\n",
    "            results.append(((category, emotion), r2))\n",
    "        else:\n",
    "            results.append(((category, emotion), None))\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"Category: {result[0][0]}, Emotion: {result[0][1]}, R2: {result[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
